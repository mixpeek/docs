---
title: "Scene Splitting"
description: "Automatically detect and extract meaningful scenes from video content"
---

<Note>
  The Scene Splitting extractor identifies natural scene boundaries in video content, enabling more precise content organization and search at the scene level rather than processing entire videos as a single unit.
</Note>

## What is Scene Splitting?

Scene splitting is a specialized feature extractor that automatically identifies natural scene boundaries in video content. It breaks down videos into semantically meaningful segments, allowing for more granular processing, feature extraction, and retrieval.

<CardGroup cols={2}>
  <Card title="Scene Detection" icon="film">
    Automatically identify natural scene transitions based on visual changes and content shifts
  </Card>
  
  <Card title="Segment Processing" icon="scissors">
    Extract features from individual scenes rather than the entire video, enabling more precise search and retrieval
  </Card>
</CardGroup>

## Key Benefits

<AccordionGroup>
  <Accordion title="Improved Search Precision" icon="bullseye">
    Search and retrieve specific scenes rather than entire videos, leading to more relevant results when users are looking for specific content moments.
  </Accordion>

  <Accordion title="Efficient Processing" icon="bolt">
    Process only the most relevant or representative frames from each scene rather than every frame in a video, reducing computational resources.
  </Accordion>

  <Accordion title="Better Content Organization" icon="folder-tree">
    Organize video content at a more granular level, making it easier to manage and retrieve specific sections of lengthy videos.
  </Accordion>

  <Accordion title="Enhanced Feature Extraction" icon="magnifying-glass">
    Extract more meaningful features by focusing on coherent scenes rather than potentially mixing content from different contexts.
  </Accordion>
</AccordionGroup>

## Configuration

<CodeGroup>
  ```json Basic Configuration
  {
    "feature_extractors": [
      {
        "scene_split": {
          "enabled": true
        }
      }
    ]
  }
  ```

  ```json Advanced Configuration
  {
    "feature_extractors": [
      {
        "scene_split": {
          "enabled": true,
          "threshold": 0.35,           // Scene change sensitivity (0-1)
          "min_scene_length": 2.5,     // Minimum scene duration in seconds
          "max_scene_length": 60,      // Maximum scene duration in seconds
          "keyframe_method": "content", // How to select the keyframe: "content", "middle", "first"
          "extract_features": true      // Whether to extract additional features for each scene
        }
      }
    ]
  }
  ```
</CodeGroup>

## Configuration Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `enabled` | boolean | `false` | Enable scene splitting |
| `threshold` | float | `0.3` | Sensitivity for scene change detection (0-1). Higher values require more significant changes for scene detection |
| `min_scene_length` | float | `2.0` | Minimum scene duration in seconds |
| `max_scene_length` | float | `120.0` | Maximum scene duration in seconds (will force-split longer scenes) |
| `keyframe_method` | string | `"content"` | Method to select the keyframe for each scene: "content" (most representative), "middle", or "first" |
| `extract_features` | boolean | `true` | Whether to automatically extract features from detected scenes |

## Processing Flow

<Frame>
  ```mermaid
  graph TD
    A[Input Video] --> B[Scene Detection]
    B --> C[Scene Segmentation]
    C --> D[Keyframe Selection]
    D --> E[Feature Extraction]
    E --> F[Document Creation]
    F --> G[Collection Storage]
    style A fill:#FC5185,stroke:#333
    style G fill:#FC5185,stroke:#333
  ```
</Frame>

## Document Structure

When scene splitting is enabled, each detected scene becomes its own document in the collection, with scene-specific metadata:

<Frame>
  ```json
  {
    "document_id": "doc_abc123",
    "collection_id": "col_xyz789",
    "source_object_id": "obj_def456",
    
    // Scene metadata
    "scene": {
      "index": 3,               // Scene index in the video (0-based)
      "start_time": 42.5,       // Start timestamp in seconds
      "end_time": 68.2,         // End timestamp in seconds
      "duration": 25.7,         // Scene duration in seconds
      "keyframe_time": 55.2,    // Timestamp of the keyframe
      "keyframe_url": "https://storage.example.com/keyframes/abc123.jpg",
      "confidence": 0.87        // Confidence score of scene boundary detection
    },
    
    // Video metadata
    "video": {
      "filename": "company_event_2023.mp4",
      "total_duration": 1258.4,  // Total video duration in seconds
      "total_scenes": 17,        // Total number of detected scenes
      "width": 1920,
      "height": 1080,
      "fps": 30
    },
    
    // Extracted features from the scene
    "detected_objects": ["person", "podium", "screen", "audience"],
    "transcript": "In this quarter, we've seen a 15% growth in our enterprise segment...",
    "scene_description": "A presenter speaking at a podium with slides visible on a screen",
    
    // System metadata
    "__fully_enriched": true,
    "__missing_features": [],
    "__pipeline_version": 1,
    
    // Timestamps
    "created_at": "2023-07-10T14:22:00Z",
    "updated_at": "2023-07-10T14:22:00Z"
  }
  ```
</Frame>

## Additional Feature Extraction

When `extract_features` is enabled, the following additional processing can be applied to each scene:

<CardGroup cols={3}>
  <Card title="Embed Keyframe" icon="image">
    Generate vector embeddings from the scene's keyframe for visual search
  </Card>
  
  <Card title="Transcribe Audio" icon="waveform">
    Extract and transcribe speech from the scene's audio track
  </Card>

  <Card title="Detect Objects" icon="object-group">
    Identify objects and entities present in the keyframe or throughout the scene
  </Card>
</CardGroup>

To configure these additional extractions, combine scene splitting with other extractors:

```json
{
  "feature_extractors": [
    {
      "scene_split": {
        "enabled": true,
        "threshold": 0.3
      },
      "embed": [
        {
          "type": "url",  // Will use the scene keyframe
          "embedding_model": "multimodal"
        }
      ],
      "transcribe": { "enabled": true },
      "detect": {
        "objects": { "enabled": true }
      },
      "describe": { "enabled": true }
    }
  ]
}
```

## Search and Retrieval

Scene splitting creates individual documents for each scene, enabling more granular search and retrieval:

```python
from mixpeek import Mixpeek

mp = Mixpeek(api_key="YOUR_API_KEY")

# Create a retriever that can search for specific scenes
retriever = mp.retrievers.create(
    namespace_id="ns_abc123",
    name="video-scene-search",
    description="Search for specific moments in videos",
    stages=[
        {
            "name": "vector_search",
            "type": "vector",
            "collection_id": "col_xyz789",
            "index": "multimodal",
            "limit": 20
        },
        {
            "name": "filters",
            "type": "filter",
            "input": "vector_search.results",
            "filter": {
                "video.filename": {"$regex": "company_event_.*\\.mp4"}
            },
            "limit": 10
        }
    ]
)

retriever_id = retriever["retriever_id"]

# Search for specific content within scenes
results = mp.retrievers.search(
    retriever_id=retriever_id,
    query={
        "text": "presenter discussing quarterly financial results"
    }
)

# Results will include specific scenes rather than entire videos
for result in results["results"]:
    print(f"Scene {result['scene']['index']} of {result['video']['filename']}")
    print(f"Timespan: {result['scene']['start_time']} - {result['scene']['end_time']}")
    print(f"Content: {result['scene_description']}")
    print(f"Transcript: {result['transcript'][:100]}...")
    print("---")
```

## Use Cases

<CardGroup cols={2}>
  <Card title="Educational Content" icon="graduation-cap">
    Break down lectures into topic-specific segments for more targeted learning experiences
  </Card>
  
  <Card title="Media Production" icon="video">
    Automatically catalog footage by scenes for easier editing and content reuse
  </Card>

  <Card title="Corporate Communications" icon="building">
    Segment long corporate videos into searchable scenes for finding specific information
  </Card>
  
  <Card title="Entertainment" icon="film">
    Create more precise search experiences for finding specific moments in shows or movies
  </Card>
</CardGroup>

## Best Practices

<Steps>
  <Step title="Tune Threshold Parameter">
    Adjust the `threshold` parameter based on your content. Lower values create more scenes (more sensitive to changes), while higher values result in fewer, longer scenes.
  </Step>
  <Step title="Set Appropriate Scene Lengths">
    Configure `min_scene_length` and `max_scene_length` based on your content type. For fast-paced content, shorter minimums work better.
  </Step>
  <Step title="Choose the Right Keyframe Method">
    Use "content" for the most representative frame, "middle" for consistent positioning, or "first" for immediate context.
  </Step>
  <Step title="Combine with Other Extractors">
    Leverage scene splitting with other extractors like transcription and object detection to create rich, searchable scene documents.
  </Step>
</Steps>

<Warning>
  Scene splitting works best with well-produced content that has clear visual transitions. User-generated content with frequent camera movements may result in over-segmentation unless the threshold is adjusted.
</Warning>

## API Reference

For more details on implementing scene splitting, see our [Pipelines API Reference](/api-reference/pipelines/create-pipeline). 