---
title: "Feature Extractors"
description: "Configure and customize multimodal feature extraction for different content types"
---

<Note>
  Feature extractors allow you to define how content is processed and what information is extracted, with support for text, images, video, and audio content.
</Note>

## What are Feature Extractors?

Feature extractors are specialized components that process different types of content to extract specific features, metadata, and embeddings. They are the core building blocks of processing pipelines, determining what information is extracted from your multimodal content.

<CardGroup cols={3}>
  <Card title="Text Processing" icon="font">
    - Text embedding
    - Language detection
    - Sentiment analysis
    - Named entity recognition
  </Card>
  
  <Card title="Image Analysis" icon="image">
    - Object detection
    - Scene classification
    - Face recognition
    - OCR extraction
  </Card>

  <Card title="Video Processing" icon="video">
    - Frame analysis
    - Transcription
    - Scene detection
    - Motion tracking
  </Card>
  
  <Card title="Audio Processing" icon="waveform">
    - Speech transcription
    - Audio embedding
    - Speaker identification
    - Sound classification
  </Card>
</CardGroup>

## Feature Extractor Configuration

<CodeGroup>
  ```json Basic Extraction
  {
    "feature_extractors": [
      {
        "embed": [
          {
            "type": "text",
            "embedding_model": "default"
          }
        ]
      }
    ]
  }
  ```

  ```json Advanced Video Processing
  {
    "feature_extractors": [
      {
        "interval_sec": 15,
        "read": { "enabled": true },
        "embed": [
          {
            "type": "url",
            "embedding_model": "multimodal"
          },
          {
            "type": "text",
            "value": "describe this scene",
            "embedding_model": "text"
          }
        ],
        "transcribe": { "enabled": true },
        "describe": { "enabled": true },
        "detect": {
          "faces": {
            "confidence_threshold": 0.8,
            "enabled": true
          },
          "objects": {
            "enabled": true
          }
        }
      }
    ]
  }
  ```
  
  ```json PDF Document Processing
  {
    "feature_extractors": [
      {
        "read": { "enabled": true },
        "embed": [
          {
            "type": "text",
            "embedding_model": "text"
          }
        ],
        "summarize": { "enabled": true },
        "classify": {
          "topics": { "enabled": true },
          "sentiment": { "enabled": true }
        },
        "extract": {
          "entities": { "enabled": true },
          "keywords": { "enabled": true }
        }
      }
    ]
  }
  ```
</CodeGroup>

## Available Extractors

<AccordionGroup>
  <Accordion title="Embedding Extractors" icon="network-wired">
    | Extractor | Description | Output |
    |-----------|-------------|--------|
    | `embed` | Generate vector embeddings | Vector representations for semantic search |
    | `keyword` | Extract SPLADE sparse embeddings | Sparse lexical embeddings |
  </Accordion>

  <Accordion title="Text Extractors" icon="font">
    | Extractor | Description | Output |
    |-----------|-------------|--------|
    | `read` | Extract text from documents | Full text content |
    | `summarize` | Generate text summaries | Concise text summary |
    | `classify` | Classify content into categories | Topic and category labels |
    | `extract` | Extract entities and keywords | Named entities and key phrases |
  </Accordion>

  <Accordion title="Visual Extractors" icon="camera">
    | Extractor | Description | Output |
    |-----------|-------------|--------|
    | `detect` | Detect objects and faces | Bounding boxes and labels |
    | `ocr` | Extract text from images | Text and positions |
    | `describe` | Generate image descriptions | Natural language description |
    | `segment` | Segment images into regions | Pixel masks and labels |
  </Accordion>

  <Accordion title="Audio/Video Extractors" icon="film">
    | Extractor | Description | Output |
    |-----------|-------------|--------|
    | `transcribe` | Convert speech to text | Text transcript with timestamps |
    | `scene_split` | Detect scene boundaries | Scene timestamps and keyframes |
    | `audio_classify` | Classify audio content | Sound classifications |
    | `speaker_diarize` | Identify different speakers | Speaker segments and IDs |
  </Accordion>
</AccordionGroup>

## Common Extractor Options

<Tabs>
  <Tab title="Embedding Options">
    ```json
    "embed": [
      {
        "type": "text",              // Input type: "text", "url", "image", "video"
        "value": "Optional text",    // Text to embed (for text type)
        "embedding_model": "text",   // Model to use: "text", "multimodal", "image"
        "field": "content.body"      // Optional field path to extract value from
      }
    ]
    ```

    <Note>
      Multiple embed configurations can be specified in a single extractor. If they use the same model, the embeddings will be averaged.
    </Note>
  </Tab>
  
  <Tab title="Detection Options">
    ```json
    "detect": {
      "objects": {
        "enabled": true,
        "confidence_threshold": 0.5,  // Minimum confidence score (0-1)
        "max_results": 20             // Maximum number of detections
      },
      "faces": {
        "enabled": true,
        "confidence_threshold": 0.7,
        "extract_embeddings": true    // Whether to extract face embeddings
      }
    }
    ```
  </Tab>
  
  <Tab title="Video Options">
    ```json
    {
      "interval_sec": 10,           // Process a frame every 10 seconds
      "interval_frame": 300,        // Or process every 300th frame
      "max_duration": 3600,         // Maximum video duration to process (seconds)
      "transcribe": {
        "enabled": true,
        "language": "auto",         // Target language or "auto"
        "include_speaker": true     // Enable speaker diarization
      },
      "scene_split": {
        "enabled": true,
        "threshold": 0.3,           // Scene change sensitivity (0-1)
        "min_scene_length": 2       // Minimum scene duration in seconds
      }
    }
    ```
  </Tab>
</Tabs>

## Processing Flow

<Frame>
  ```mermaid
  graph TD
    A[Raw Object] --> B[Feature Extractors]
    B -- Text --> C[Text Processing]
    B -- Images --> D[Image Processing]
    B -- Video --> E[Video Processing]
    B -- Audio --> F[Audio Processing]
    C --> G[Text Features]
    D --> H[Image Features]
    E --> I[Video Features]
    F --> J[Audio Features]
    G --> K[Document]
    H --> K
    I --> K
    J --> K
    K --> L[Collection]
    style A fill:#FC5185,stroke:#333
    style L fill:#FC5185,stroke:#333
  ```
</Frame>

## Pipeline Integration

Feature extractors are configured as part of a pipeline definition:

```python
from mixpeek import Mixpeek

mp = Mixpeek(api_key="YOUR_API_KEY")

# Create a pipeline with feature extractors
pipeline = mp.pipelines.create(
    namespace_id="ns_abc123",
    name="product-processing",
    description="Extract features from product content",
    input_bucket_id="bkt_def456",
    output_collection_ids=["col_mno345"],
    feature_extractors=[
        {
            # Image processing configuration
            "embed": [
                {
                    "type": "url",
                    "embedding_model": "multimodal"
                }
            ],
            "detect": {
                "objects": {"enabled": True}
            },
            "describe": {"enabled": True}
        },
        {
            # Text processing configuration
            "read": {"enabled": True},
            "embed": [
                {
                    "type": "text",
                    "embedding_model": "text"
                }
            ],
            "classify": {
                "topics": {"enabled": True}
            }
        }
    ]
)

pipeline_id = pipeline["pipeline_id"]
print(f"Created pipeline: {pipeline_id}")
```

## Dependency Management

Some feature extractors depend on the output of others. Mixpeek automatically manages these dependencies:

<Frame>
  ```mermaid
  graph TD
    A[read] --> B[embed]
    A --> C[summarize]
    A --> D[classify]
    A --> E[extract]
    F[detect] --> G[describe]
    H[transcribe] --> I[embed]
    style A fill:#FC5185,stroke:#333
    style F fill:#FC5185,stroke:#333
    style H fill:#FC5185,stroke:#333
  ```
</Frame>

## Best Practices

<Steps>
  <Step title="Choose the Right Extractors">
    Select only the extractors needed for your use case to optimize processing time and resource usage
  </Step>
  <Step title="Configure Thresholds">
    Adjust confidence thresholds based on your precision/recall requirements
  </Step>
  <Step title="Match Embeddings to Retrieval">
    Ensure embedding models match your retrieval strategy and query patterns
  </Step>
  <Step title="Consider Processing Time">
    Be mindful of processing-intensive extractors when working with large files or videos
  </Step>
</Steps>

## Example Use Cases

<CardGroup cols={2}>
  <Card title="E-commerce Product Catalog" icon="shopping-cart">
    Extract product features, descriptions, and visual attributes for semantic search and filtering
    
    ```json
    {
      "embed": [
        { "type": "url", "embedding_model": "multimodal" }
      ],
      "detect": { "objects": { "enabled": true } },
      "describe": { "enabled": true },
      "read": { "enabled": true },
      "extract": { "keywords": { "enabled": true } }
    }
    ```
  </Card>
  
  <Card title="Media Content Library" icon="photo-film">
    Process videos and images for content discovery and scene-based retrieval
    
    ```json
    {
      "interval_sec": 5,
      "embed": [
        { "type": "url", "embedding_model": "multimodal" }
      ],
      "scene_split": { "enabled": true },
      "transcribe": { "enabled": true },
      "detect": {
        "faces": { "enabled": true },
        "objects": { "enabled": true }
      }
    }
    ```
  </Card>
</CardGroup>

## Limitations

<Warning>
  Be aware of these technical constraints:
  - Maximum video duration: 4 hours
  - Maximum file size: 2GB
  - Processing timeout: 30 minutes
  - Rate limits apply to extraction requests
</Warning>

For more advanced scenarios, check out specialized extractors like [Scene Splitting](/processing/extractors/scene-splitting) and explore options for [Model Tuning](/processing/model-tuning). 