---
title: "Clusters"
description: "Discover structure, label groups, and enrich collections with clustering"
---

<Note>
  Clustering groups similar documents to power discovery, analytics visuals, and taxonomy bootstrapping. Mixpeek runs clustering as an engine pipeline, stores artifacts (Parquet) for scale, and exposes APIs to create, execute, list, stream, and apply enrichments. **New:** Automated triggers let you schedule clustering with cron expressions, intervals, events, or conditions—keeping your clusters fresh without manual intervention.
</Note>

## Overview

<CardGroup cols={2}>
  <Card title="Structure Discovery" icon="diagram-project">
    Find groups in one or more collections; optional hierarchical metadata
  </Card>
  
  <Card title="Scalable Visualization" icon="chart-scatter-3d">
    Landmark-based UMAP generates 2D/3D coords for millions of points
  </Card>
  
  <Card title="Artifacts & Scale" icon="box-archive">
    Results saved as Parquet (centroids, members) for WebGL/Arrow pipelines
  </Card>
  
  <Card title="LLM Labeling" icon="wand-magic-sparkles">
    Optionally name clusters, summaries, and keywords
  </Card>
  
  <Card title="Enrichment" icon="plus">
    Write back cluster_id membership or create derived collections
  </Card>

  <Card title="Automated Triggers" icon="clock">
    Schedule clustering with cron, intervals, events, or conditions
  </Card>
  
  <Card title="Event-Driven" icon="bolt">
    Auto-recluster when documents added or data changes significantly
  </Card>
  
  <Card title="Distributed Processing" icon="network-wired">
    Ray map_batches processes millions of points with bounded memory
  </Card>
</CardGroup>

## How it works

<Frame>
  ```mermaid
  graph TD
    subgraph Execution["Execution Modes"]
      MAN[Manual Execute API]
      TRIG[Automated Triggers]
    end
    
    MAN --> PRE[Preprocess]
    TRIG -->|Cron/Interval/Event/Conditional| PRE
    
    SRC[Source Collections] --> PRE
    PRE --> CL[Clustering Algorithm]
    CL --> POST[Postprocess & LLM Labeling]
    POST --> ART[Artifacts Parquet]
    POST --> ENR[Optional Enrichment]
    
    ENR -.->|Write cluster_id| SRC
    POST -.->|Webhook events| WH[Webhooks]
  ```
</Frame>

<Steps>
  <Step title="Trigger or Execute">
    Start clustering via manual API call or automated trigger (cron, interval, event, conditional)
  </Step>
  <Step title="Preprocess">
    Optional normalization and dimensionality reduction (UMAP, tSNE, PCA)
  </Step>
  <Step title="Cluster">
    Algorithms like KMeans, DBSCAN, HDBSCAN, Agglomerative, Spectral, GMM, Mean Shift, OPTICS
  </Step>
  <Step title="Postprocess">
    Compute centroids and stats; optional LLM labeling and hierarchical metadata
  </Step>
  <Step title="Persist & Notify">
    Parquet artifacts saved per run_id; webhook events emitted for monitoring
  </Step>
  <Step title="Enrich (optional)">
    Write cluster_id membership and labels back to documents
  </Step>
</Steps>

## Multimodal example

<Frame>
  ```mermaid
  graph TD
    subgraph Sources
      IMG[Images]
      VID[Videos]
      AUD[Audio]
      TXT[Text]
    end

    IMG --> FE[Feature Extraction - CLIP, Wav2Vec, e5]
    VID --> FE
    AUD --> FE
    TXT --> FE

    FE --> VEC[Vectors]
    VEC --> PRE[Preprocess - normalize and reduce]
    PRE --> CL[Cluster - KMeans or HDBSCAN]
    CL --> ART[Artifacts - Parquet]
    ART --> ENR[Enrich - cluster_id and labels]
    ENR -. optional .-> TAX[Derived Taxonomy]
  ```
</Frame>

## Create a cluster definition

- **API**: Create Cluster
- **Method**: POST
- **Path**: `/v1/clusters`
- **Reference**: [API Reference](/api-reference/clusters/create-cluster)

```bash
curl -X POST https://api.mixpeek.com/v1/clusters \
  -H "Authorization: Bearer $API_KEY" \
  -H "X-Namespace: ns_123" \
  -H "Content-Type: application/json" \
  -d '{
    "collection_ids": ["col_products_v1"],
    "cluster_name": "products_clip_hdbscan",
    "cluster_type": "vector",
    "vector_config": {
      "feature_extractor_name": "clip_vit_l_14",
      "clustering_method": "hdbscan",
      "hdbscan_parameters": {"min_cluster_size": 10, "min_samples": 5}
    },
    "llm_labeling": {"enabled": true, "model_name": "gpt-4"}
  }'
```

## Execute clustering

- **API**: Execute Clustering
- **Method**: POST
- **Path**: `/v1/clusters/execute`
- **Reference**: [API Reference](/api-reference/clusters/execute-clustering)

```bash
curl -X POST https://api.mixpeek.com/v1/clusters/execute \
  -H "Authorization: Bearer $API_KEY" \
  -H "X-Namespace: ns_123" \
  -H "Content-Type: application/json" \
  -d '{
    "collection_ids": ["col_products_v1"],
    "config": {
      "algorithm": "kmeans",
      "algorithm_params": {"n_clusters": 8, "max_iter": 300},
      "feature_vector": {"feature_address": {"extractor": "clip_vit_l_14", "version": "1.0.0"}},
      "normalize_features": true,
      "dimensionality_reduction": {"method": "umap", "n_components": 2}
    },
    "sample_size": 10000,
    "store_results": true,
    "include_members": false,
    "compute_metrics": true,
    "save_artifacts": true
  }'
```

Response includes `run_id`, metrics, and centroid summaries. Artifacts are written under a per‑run S3 prefix.

## Automated clustering with triggers

Instead of manually executing clustering, you can define **triggers** that automatically run clustering jobs based on schedules, events, or conditions. This ensures your clusters stay fresh without manual intervention.

<Info>
  Triggers are perfect for production workflows where data constantly changes—nightly reclustering, event-driven updates, or condition-based refreshes.
</Info>

### Why use triggers?

<CardGroup cols={2}>
  <Card title="Stay Fresh" icon="arrows-rotate">
    Keep clusters up-to-date as new documents arrive
  </Card>
  
  <Card title="Hands-Off" icon="hand-peace">
    Set once, runs automatically—no manual execution needed
  </Card>
  
  <Card title="Resource Efficient" icon="gauge-high">
    Schedule during off-peak hours or when data changes significantly
  </Card>
  
  <Card title="Production Ready" icon="shield-check">
    Built-in failure handling, webhooks, and execution history
  </Card>
</CardGroup>

### Trigger types

Mixpeek supports four trigger types, each suited for different use cases:

<Tabs>
  <Tab title="Cron Triggers">
    **Execute clustering at specific times using cron expressions.**

    **Perfect for:**
    - Nightly reclustering at 2am
    - Weekly batch processing every Sunday
    - Month-end analytics

    ```bash
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123" \
      -H "Content-Type: application/json" \
      -d '{
        "execution_config": {
          "collection_ids": ["col_products"],
          "config": {
            "algorithm": "kmeans",
            "algorithm_params": {"n_clusters": 10}
          }
        },
        "trigger_type": "cron",
        "schedule_config": {
          "cron_expression": "0 2 * * *",
          "timezone": "America/New_York"
        },
        "description": "Nightly product clustering at 2am EST"
      }'
    ```

    **Common cron expressions:**
    - `"0 2 * * *"` - Daily at 2:00am
    - `"0 */6 * * *"` - Every 6 hours
    - `"0 0 * * 0"` - Every Sunday at midnight
    - `"30 14 1 * *"` - First day of month at 2:30pm

  </Tab>
  
  <Tab title="Interval Triggers">
    **Execute clustering at fixed time intervals.**

    **Perfect for:**
    - Continuous monitoring with hourly updates
    - Regular refresh every 30 minutes
    - Periodic batch processing

    ```bash
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123" \
      -H "Content-Type: application/json" \
      -d '{
        "cluster_id": "cl_existing_cluster",
        "trigger_type": "interval",
        "schedule_config": {
          "interval_seconds": 3600,
          "start_immediately": false
        },
        "description": "Hourly cluster refresh"
      }'
    ```

    **Minimum interval:** 300 seconds (5 minutes)

  </Tab>
  
  <Tab title="Event Triggers">
    **Execute clustering when specific events occur.**

    **Perfect for:**
    - Cluster after 1,000 new documents added
    - Re-cluster when batch import completes
    - Update clusters on significant data changes

    ```bash
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123" \
      -H "Content-Type: application/json" \
      -d '{
        "execution_config": {
          "collection_ids": ["col_products"],
          "config": {"algorithm": "hdbscan"}
        },
        "trigger_type": "event",
        "schedule_config": {
          "event_type": "documents_added",
          "event_threshold": 1000,
          "collection_id": "col_products",
          "cooldown_seconds": 1800
        },
        "description": "Cluster after every 1000 new products"
      }'
    ```

    **Event types:**
    - `documents_added` - New documents added to collection
    - `documents_updated` - Documents updated in collection
    - `batch_completed` - Batch processing completed

    **Cooldown:** Prevents rapid repeated triggering (default: 300 seconds)

  </Tab>
  
  <Tab title="Conditional Triggers">
    **Execute clustering when conditions are met (advanced).**

    **Perfect for:**
    - Cluster when embedding drift exceeds threshold
    - Re-cluster when collection size doubles
    - Update when cluster quality degrades

    ```bash
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123" \
      -H "Content-Type: application/json" \
      -d '{
        "execution_config": {
          "collection_ids": ["col_products"],
          "config": {"algorithm": "kmeans"}
        },
        "trigger_type": "conditional",
        "schedule_config": {
          "condition_type": "drift",
          "threshold": 0.15,
          "metric": "embedding_drift",
          "check_interval_seconds": 3600
        },
        "description": "Cluster when drift exceeds 15%"
      }'
    ```

    **Condition types:**
    - `drift` - Vector embedding drift detection
    - `volume` - Collection size changes
    - `quality` - Cluster quality metrics
    - `custom` - User-defined webhook-based conditions

  </Tab>
</Tabs>

### Managing triggers

<AccordionGroup>
  <Accordion title="Pause and resume triggers" icon="pause">
    Temporarily stop a trigger without deleting it:

    ```bash
    # Pause trigger
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers/trig_abc123/pause \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123"

    # Resume trigger
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers/trig_abc123/resume \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123"
    ```

    When resumed, the next execution time is recalculated from the current time.
  </Accordion>
  
  <Accordion title="Update trigger schedules" icon="pen">
    Modify schedule configuration without recreating the trigger:

    ```bash
    curl -X PATCH https://api.mixpeek.com/v1/clusters/triggers/trig_abc123 \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123" \
      -H "Content-Type: application/json" \
      -d '{
        "schedule_config": {
          "cron_expression": "0 3 * * *"
        },
        "description": "Updated to 3am"
      }'
    ```

    **Note:** Trigger type is immutable—delete and recreate to change type.
  </Accordion>
  
  <Accordion title="View execution history" icon="clock-rotate-left">
    Track all executions of a trigger with detailed metrics:

    ```bash
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers/trig_abc123/history \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123" \
      -H "Content-Type: application/json" \
      -d '{
        "offset": 0,
        "limit": 50,
        "status_filter": "completed"
      }'
    ```

    Response includes:
    - Job IDs and execution times
    - Status (completed/failed)
    - Execution duration
    - Cluster count and document count
  </Accordion>
  
  <Accordion title="List and filter triggers" icon="list">
    Find triggers by cluster, type, or status:

    ```bash
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers/list \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123" \
      -H "Content-Type: application/json" \
      -d '{
        "cluster_id": "cl_xyz789",
        "trigger_type": "cron",
        "status": "active",
        "offset": 0,
        "limit": 50
      }'
    ```
  </Accordion>
</AccordionGroup>

### Trigger lifecycle and status

<Steps>
  <Step title="Active">
    Trigger is enabled and will fire according to schedule. This is the normal operating state.
  </Step>
  <Step title="Paused">
    Trigger is temporarily disabled but retains configuration. Use pause endpoint to enter this state.
  </Step>
  <Step title="Failed">
    Trigger automatically disabled after 5 consecutive failures. Requires manual resume after fixing issues.
  </Step>
  <Step title="Disabled">
    Trigger soft-deleted via DELETE endpoint. No longer executes but history is preserved.
  </Step>
</Steps>

### Failure handling and recovery

Triggers include built-in resilience:

- **Single failures:** Logged but trigger continues
- **Consecutive failures:** Tracked in trigger metadata
- **5 consecutive failures:** Trigger status changes to `failed`, requires manual resume
- **Webhook notifications:** Sent for each failure with error details

**Recovery steps:**
1. Check `last_execution_error` field via GET endpoint
2. Fix underlying issue (e.g., invalid config, missing resources)
3. Update trigger if needed with PATCH endpoint
4. Resume trigger with POST to `/resume` endpoint

### Webhook events

Triggers emit lifecycle events you can subscribe to:

- `trigger.created` - New trigger created
- `trigger.fired` - Trigger fired and created clustering job
- `trigger.execution.completed` - Clustering job completed successfully
- `trigger.execution.failed` - Clustering job failed
- `trigger.paused` / `trigger.resumed` - State changes
- `trigger.deleted` - Trigger removed

Subscribe via the [webhooks API](/operations/webhooks) to build custom workflows.

### Example: Complete automation workflow

Here's a production-ready setup for a product catalog:

<Steps>
  <Step title="Create nightly reclustering">
    ```bash
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_products" \
      -H "Content-Type: application/json" \
      -d '{
        "execution_config": {
          "collection_ids": ["col_catalog"],
          "config": {
            "algorithm": "kmeans",
            "algorithm_params": {"n_clusters": 20},
            "normalize_features": true,
            "llm_labeling": {"enabled": true}
          }
        },
        "trigger_type": "cron",
        "schedule_config": {
          "cron_expression": "0 2 * * *",
          "timezone": "America/New_York"
        },
        "description": "Daily product reclustering with labels"
      }'
    ```
  </Step>
  
  <Step title="Add event-based trigger for rapid changes">
    ```bash
    curl -X POST https://api.mixpeek.com/v1/clusters/triggers \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_products" \
      -H "Content-Type: application/json" \
      -d '{
        "execution_config": {
          "collection_ids": ["col_catalog"],
          "config": {"algorithm": "hdbscan"}
        },
        "trigger_type": "event",
        "schedule_config": {
          "event_type": "documents_added",
          "event_threshold": 500,
          "cooldown_seconds": 3600
        },
        "description": "Quick recluster after 500 new products"
      }'
    ```
  </Step>
  
  <Step title="Monitor with webhooks">
    Subscribe to `trigger.execution.completed` events to:
    - Track clustering performance over time
    - Alert on cluster count changes
    - Trigger downstream enrichment pipelines
  </Step>
  
  <Step title="Apply enrichment automatically">
    When trigger completes, use enrichment API to write `cluster_id` back to documents for filtering and faceting.
  </Step>
</Steps>

### Quotas and limits

- **Max active triggers:** 50 per namespace
- **Min interval:** 300 seconds (5 minutes)
- **Default cooldown:** 300 seconds (5 minutes)
- **Polling interval:** 60 seconds
- **Max consecutive failures:** 5 (auto-disables trigger)

## Stream artifacts for UI

- **API**: Stream Cluster Data
- **Method**: POST
- **Path**: `/v1/clusters/{cluster_id}/data`
- **Reference**: [API Reference](/api-reference/clusters/stream-cluster-data)

```bash
curl -X POST https://api.mixpeek.com/v1/clusters/CLUSTER_ID/data \
  -H "Authorization: Bearer $API_KEY" \
  -H "X-Namespace: ns_123" \
  -H "Content-Type: application/json" \
  -d '{
    "cluster_id": "cl_123",
    "include_centroids": true,
    "include_members": true,
    "limit": 1000,
    "offset": 0
  }'
```

Use this to load centroids and members for visualizations (2D/3D reductions, partitions by cluster_id).

## Scalable visualization with dimensionality reduction

For large clusters, Mixpeek provides **engine-powered visualization** that generates 2D/3D coordinates for millions of points using landmark-based dimensionality reduction. This enables interactive scatter plots, WebGL renderers, and spatial exploration at scale.

<Info>
  Visualization uses landmark-based UMAP with Nyström approximation—fitting on 2% of points and interpolating the rest. This scales to millions of points while maintaining quality.
</Info>

### Why use engine visualization?

<CardGroup cols={2}>
  <Card title="Scales to Millions" icon="chart-scatter">
    Landmark-based DR processes 1M+ points in minutes
  </Card>
  
  <Card title="Distributed Processing" icon="network-wired">
    Ray map_batches processes 5k points per batch across cluster
  </Card>
  
  <Card title="Cached & Reusable" icon="database">
    S3-cached coordinates with signature-based keys—no redundant computation
  </Card>
  
  <Card title="Production Ready" icon="rocket">
    Hard cap at 10k forces engine delegation for safety
  </Card>
</CardGroup>

### How it works

<Frame>
  ```mermaid
  graph TD
    REQ[Request Visualization] --> CHECK{Point Count}
    CHECK -->|< 10k| MANUAL[Manual Generation Required]
    CHECK -->|> 10k| ENGINE[Delegate to Engine]
    
    ENGINE --> SIG[Compute Signature]
    SIG --> CACHE{Cached?}
    CACHE -->|Yes| LOAD[Load from S3]
    CACHE -->|No| FIT[Fit on Landmarks 2%]
    
    FIT --> TRANSFORM[Transform with Ray]
    TRANSFORM --> SAVE[Save to S3]
    SAVE --> LOAD
    
    LOAD --> COORDS[Return Coordinates]
    COORDS --> VIZ[Render Visualization]
  ```
</Frame>

<Steps>
  <Step title="Request visualization">
    Client requests coordinates via API. If cluster has >10k points, automatically delegates to engine.
  </Step>
  <Step title="Compute signature">
    Engine generates cache key from cluster ID, method, parameters—ensures idempotence.
  </Step>
  <Step title="Check cache">
    If coordinates already exist in S3 with matching signature, skip computation and return cached URLs.
  </Step>
  <Step title="Fit on landmarks">
    Sample 2% of points (capped at 50k) as landmarks. Fit UMAP or incremental PCA on landmarks only.
  </Step>
  <Step title="Transform with Ray">
    Use Ray map_batches to apply dimensionality reduction to all points in 5k batches—distributed and memory-efficient.
  </Step>
  <Step title="Cache and return">
    Save coordinates to S3 as Parquet. Return URLs to API, which loads and serves to client.
  </Step>
</Steps>

### Generate visualization

For datasets with >10k points, visualization is generated automatically when you request artifacts. For smaller datasets, call the engine endpoint directly.

<Tabs>
  <Tab title="Automatic (>10k points)">
    ```bash
    # For large clusters, visualization is automatic
    curl https://api.mixpeek.com/v1/clusters/cl_abc123/artifacts?include_members=true \
      -H "Authorization: Bearer $API_KEY" \
      -H "X-Namespace: ns_123"
    ```
    
    **Response includes coordinates:**
    ```json
    {
      "centroids": [...],
      "members": [
        {
          "point_id": "doc_1",
          "cluster_id": "cluster_0",
          "x": 1.5,
          "y": 2.3,
          "payload": {}
        }
      ]
    }
    ```
  </Tab>
  
  <Tab title="Manual (<10k points)">
    ```bash
    # Generate visualization via engine
    curl -X POST https://engine.mixpeek.com/clusters/visualization \
      -H "Content-Type: application/json" \
      -d '{
        "cluster_id": "cl_abc123",
        "internal_id": "int_xyz",
        "namespace_id": "ns_789",
        "method": "umap",
        "n_components": 2,
        "sample_pct": 0.02,
        "force_recompute": false
      }'
    ```
    
    **Response:**
    ```json
    {
      "success": true,
      "status": {
        "cluster_id": "cl_abc123",
        "status": "completed",
        "signature": "a1b2c3d4",
        "coords_url": "s3://bucket/.../coords_a1b2c3d4.parquet",
        "num_points": 50000
      }
    }
    ```
  </Tab>
</Tabs>

### Configuration options

<AccordionGroup>
  <Accordion title="Dimensionality reduction methods" icon="chart-line">
    **UMAP (Uniform Manifold Approximation and Projection)**
    - Best for: Preserving both local and global structure
    - Quality: Excellent cluster separation
    - Speed: Moderate (60s for 1M points)
    - Use when: Visualization quality matters most
    
    **Incremental PCA**
    - Best for: Fast linear projections
    - Quality: Good for high-dimensional data
    - Speed: Fast (30s for 1M points)
    - Use when: Speed matters more than non-linear structure
    
    ```json
    {
      "method": "umap",        // or "ipca"
      "n_components": 2        // 2D or 3D
    }
    ```
  </Accordion>
  
  <Accordion title="Landmark sampling" icon="location-dot">
    Controls how many points are used as landmarks for fitting:
    
    - `sample_pct`: Percentage of points to use as landmarks (default: 0.02 = 2%)
    - `max_landmarks`: Cap on landmark count (default: 50,000)
    - `k_landmarks`: Nearest landmarks for interpolation (default: 15)
    
    **Examples:**
    - 10k points → 200 landmarks (2%)
    - 100k points → 2k landmarks (2%)
    - 1M points → 20k landmarks (2%)
    - 10M points → 50k landmarks (capped)
    
    ```json
    {
      "sample_pct": 0.02,
      "max_landmarks": 50000,
      "k_landmarks": 15
    }
    ```
    
    **Tuning guide:**
    - More landmarks = better quality, slower processing
    - Fewer landmarks = faster processing, lower quality
    - Sweet spot: 0.02 (2%) for most use cases
  </Accordion>
  
  <Accordion title="UMAP parameters" icon="sliders">
    Fine-tune UMAP behavior for different visualization needs:
    
    - `n_neighbors`: Balance local vs global structure (default: 15)
    - `min_dist`: Minimum distance between points (default: 0.1)
    - `metric`: Distance metric (default: "cosine")
    
    **Common configurations:**
    
    **Tight clusters (detail view):**
    ```json
    {
      "umap_n_neighbors": 30,
      "umap_min_dist": 0.01,
      "umap_metric": "cosine"
    }
    ```
    
    **Spread out (overview):**
    ```json
    {
      "umap_n_neighbors": 10,
      "umap_min_dist": 0.3,
      "umap_metric": "euclidean"
    }
    ```
    
    **High-dimensional embeddings:**
    ```json
    {
      "umap_n_neighbors": 20,
      "umap_min_dist": 0.1,
      "umap_metric": "cosine"  // Best for embeddings
    }
    ```
  </Accordion>
  
  <Accordion title="Caching and recomputation" icon="rotate">
    Visualization results are cached in S3 with signature-based keys:
    
    **Signature includes:**
    - Cluster ID
    - Feature name
    - DR method and parameters
    - Component count
    
    **Behavior:**
    - Same parameters → same signature → cached result
    - Different parameters → new signature → new computation
    - Set `force_recompute: true` to bypass cache
    
    ```json
    {
      "force_recompute": false  // Use cached if available
    }
    ```
    
    **Cache location:**
    ```
    s3://bucket/{internal_id}/{namespace_id}/
      engine_cluster_build/{cluster_id}/
        visualization/
          coords_{signature}.parquet
          reducer_{signature}.pkl
    ```
  </Accordion>
</AccordionGroup>

### Performance characteristics

<Tabs>
  <Tab title="Processing Times">
    Approximate times for landmark-based UMAP on typical hardware:
    
    | Dataset Size | Landmarks | Fit Time | Transform Time | Total |
    |--------------|-----------|----------|----------------|-------|
    | 1k points    | 100       | 2s       | 0.5s          | 2.5s  |
    | 10k points   | 500       | 5s       | 2s            | 7s    |
    | 100k points  | 2k        | 15s      | 10s           | 25s   |
    | 1M points    | 20k       | 60s      | 60s           | 2m    |
    | 10M points   | 50k       | 120s     | 300s          | 7m    |
    
    **Notes:**
    - Fit time scales with landmark count squared
    - Transform time scales with point count × k_landmarks
    - Times assume 16-core machine
  </Tab>
  
  <Tab title="Memory Usage">
    Memory consumption during visualization processing:
    
    | Dataset Size | Landmarks | Peak Memory |
    |--------------|-----------|-------------|
    | 1k points    | 100       | ~50 MB      |
    | 10k points   | 500       | ~200 MB     |
    | 100k points  | 2k        | ~500 MB     |
    | 1M points    | 20k       | ~2 GB       |
    | 10M points   | 50k       | ~10 GB      |
    
    **Ray batching keeps memory bounded:**
    - Processes 5k points per batch
    - ~100-500 MB per batch
    - Distributed across cluster nodes
  </Tab>
  
  <Tab title="Quality Metrics">
    Landmark-based UMAP preserves structure well:
    
    **Trust & Continuity (higher is better):**
    - 2% landmarks: 0.92-0.95 (excellent)
    - 1% landmarks: 0.88-0.92 (good)
    - 0.5% landmarks: 0.82-0.88 (acceptable)
    
    **Neighborhood Preservation:**
    - k=15 neighbors: 85-90% preserved
    - k=30 neighbors: 80-85% preserved
    
    **Recommendation:** Stick with 2% landmarks (default) for production use.
  </Tab>
</Tabs>

### Integration with visualization tools

<CardGroup cols={2}>
  <Card title="WebGL Scatter Plots" icon="chart-scatter-3d">
    Use Three.js, Plotly, or deck.gl to render millions of points with GPU acceleration
  </Card>
  
  <Card title="Observable Plots" icon="chart-simple">
    Load Parquet directly in Observable notebooks for interactive exploration
  </Card>
  
  <Card title="Apache Arrow" icon="table">
    Use PyArrow or Arrow.js to stream coordinates without full deserialization
  </Card>
  
  <Card title="Tile-Based Rendering" icon="grid">
    Bin coordinates into tiles for progressive loading in zoom-enabled UIs
  </Card>
</CardGroup>

**Example: Load in JavaScript**
```javascript
import { tableFromIPC } from 'apache-arrow';

// Fetch coordinates from API
const response = await fetch('/v1/clusters/cl_abc/artifacts?include_members=true');
const { members } = await response.json();

// Render with Three.js, Plotly, etc.
const scene = new THREE.Scene();
members.forEach(point => {
  const geometry = new THREE.SphereGeometry(0.05);
  const material = new THREE.MeshBasicMaterial({ 
    color: getColorForCluster(point.cluster_id) 
  });
  const sphere = new THREE.Mesh(geometry, material);
  sphere.position.set(point.x, point.y, point.z || 0);
  scene.add(sphere);
});
```

### Troubleshooting

<AccordionGroup>
  <Accordion title="Dataset has X points. Client should request visualization generation via engine." icon="triangle-exclamation">
    **Cause:** Cluster has fewer than 10k points, no pre-generated visualization.
    
    **Solution:**
    ```bash
    # Generate visualization first
    POST /clusters/visualization
    {
      "cluster_id": "cl_abc123",
      "method": "umap",
      "n_components": 2
    }
    
    # Then request artifacts
    GET /clusters/cl_abc123/artifacts?include_members=true
    ```
  </Accordion>
  
  <Accordion title="Visualization quality is poor" icon="chart-line">
    **Symptoms:** Clusters overlap, structure is unclear, points are too spread out.
    
    **Solutions:**
    1. **Increase landmarks:** Set `sample_pct: 0.03` or `0.05`
    2. **Adjust n_neighbors:** Try `20-30` for more global structure
    3. **Tighten clusters:** Set `min_dist: 0.01` for less overlap
    4. **Check metric:** Use `"cosine"` for embeddings, `"euclidean"` for raw features
    5. **Try 3D:** Set `n_components: 3` for complex structures
  </Accordion>
  
  <Accordion title="Processing is slow" icon="hourglass">
    **Symptoms:** Visualization takes too long for interactive use.
    
    **Solutions:**
    1. **Reduce landmarks:** Set `sample_pct: 0.01` (1%)
    2. **Use IPCA:** Set `method: "ipca"` for 2x speedup
    3. **Check Ray cluster:** Ensure sufficient CPUs allocated
    4. **Verify caching:** Ensure `force_recompute: false` to use cache
    5. **Increase batch size:** Larger batches = fewer overhead (tune in engine config)
  </Accordion>
  
  <Accordion title="Engine visualization failed" icon="circle-xmark">
    **Symptoms:** Engine returns error during visualization generation.
    
    **Debugging steps:**
    1. Check engine logs for detailed error message
    2. Verify `members.parquet` exists in S3 for the cluster
    3. Try with `force_recompute: true` to bypass cache
    4. Check Ray cluster health and resource availability
    5. Verify feature vectors are present in member data
    
    **Common causes:**
    - Missing or corrupted member artifacts
    - Insufficient memory for landmark count
    - Ray actor failures (check Ray dashboard)
  </Accordion>
</AccordionGroup>

## Apply cluster enrichment

- **API**: Apply Cluster Enrichment
- **Method**: POST
- **Path**: `/v1/clusters/enrich`
- **Reference**: [API Reference](/api-reference/clusters/apply-cluster-enrichment)

```bash
curl -X POST https://api.mixpeek.com/v1/clusters/enrich \
  -H "Authorization: Bearer $API_KEY" \
  -H "X-Namespace: ns_123" \
  -H "Content-Type: application/json" \
  -d '{
    "clustering_ids": ["cl_run_abc"],
    "source_collection_id": "col_products_v1",
    "target_collection_id": "col_products_enriched_v1",
    "batch_size": 1000,
    "parallelism": 4
  }'
```

This writes `cluster_id` membership (and optional labels) back to documents.

## Manage clusters

<Tabs>
  <Tab title="Cluster Operations">
    <CardGroup cols={2}>
      <Card title="Get Cluster" icon="magnifying-glass" href="/api-reference/clusters/get-cluster" />
      <Card title="List Clusters" icon="list" href="/api-reference/clusters/list-clusters" />
      <Card title="Delete Cluster" icon="trash" href="/api-reference/clusters/delete-cluster" />
      <Card title="Get Artifacts" icon="box-open" href="/api-reference/clusters/get-cluster-artifacts" />
      <Card title="Submit Job" icon="paper-plane" href="/api-reference/clusters/submit-clustering-job" />
    </CardGroup>
  </Tab>
  
  <Tab title="Trigger Management">
    <CardGroup cols={2}>
      <Card title="Create Trigger" icon="plus" href="/api-reference/clusters/triggers/create-trigger">
        Set up automated clustering schedules
      </Card>
      <Card title="List Triggers" icon="list" href="/api-reference/clusters/triggers/list-triggers">
        View all active and paused triggers
      </Card>
      <Card title="Get Trigger" icon="magnifying-glass" href="/api-reference/clusters/triggers/get-trigger">
        View trigger details and next execution
      </Card>
      <Card title="Update Trigger" icon="pen" href="/api-reference/clusters/triggers/update-trigger">
        Modify schedule or configuration
      </Card>
      <Card title="Pause Trigger" icon="pause" href="/api-reference/clusters/triggers/pause-trigger">
        Temporarily disable execution
      </Card>
      <Card title="Resume Trigger" icon="play" href="/api-reference/clusters/triggers/resume-trigger">
        Reactivate paused trigger
      </Card>
      <Card title="Delete Trigger" icon="trash" href="/api-reference/clusters/triggers/delete-trigger">
        Remove trigger permanently
      </Card>
      <Card title="Execution History" icon="clock-rotate-left" href="/api-reference/clusters/triggers/get-execution-history">
        View past runs and metrics
      </Card>
    </CardGroup>
  </Tab>
</Tabs>

## Config building blocks

<Tabs>
  <Tab title="Algorithms">
    kmeans, dbscan, hdbscan, agglomerative, spectral, gaussian_mixture, mean_shift, optics
  </Tab>
  
  <Tab title="Reduction & Normalization">
    Optional UMAP, tSNE, PCA; set <code>normalize_features</code> to true for L2/standard scaling
  </Tab>
  
  <Tab title="LLM Labeling">
    Enable labeling with provider/model and parameters to name clusters and extract keywords
  </Tab>
  
  <Tab title="Hierarchy">
    Set <code>hierarchical</code> and <code>max_hierarchy_depth</code> to compute parent_cluster_id and levels
  </Tab>
</Tabs>

## Artifacts (Parquet)

<AccordionGroup>
  <Accordion title="Centroids dataset" icon="bullseye">
    Columns include: <code>cluster_id</code>, <code>centroid_vector</code>, <code>num_members</code>, <code>variance</code>, <code>label</code>, <code>summary</code>, <code>keywords</code>, <code>feature_name</code>, <code>feature_dimensions</code>, <code>parent_cluster_id</code>, <code>hierarchy_level</code>, <code>reduction_method</code>, <code>parameters</code>, <code>algorithm</code>, <code>run_id</code>, timestamps
  </Accordion>
  <Accordion title="Members dataset" icon="users">
    Partitioned by <code>cluster_id</code>; includes <code>point_id</code>, reduced coordinates (<code>x</code>, <code>y</code>, optional <code>z</code>), and optional payload slice for filtering
  </Accordion>
</AccordionGroup>

## Best practices

<Steps>
  <Step title="Start with samples">
    Use <code>sample_size</code> for quick exploration before full runs. Test clustering algorithms and parameters on a subset before scaling up.
  </Step>
  <Step title="Automate with triggers">
    Set up cron triggers for nightly reclustering and event triggers for rapid data changes. This keeps clusters fresh without manual work.
  </Step>
  <Step title="Label pragmatically">
    Enable LLM labeling after validating cluster separation. Labels are expensive—ensure your clusters are meaningful first.
  </Step>
  <Step title="Monitor with webhooks">
    Subscribe to trigger execution events to track performance, alert on anomalies, and chain downstream workflows automatically.
  </Step>
  <Step title="Persist visuals">
    Save artifacts and stream reduced coordinates for UI at scale. Parquet format enables fast loading in WebGL and data viz tools.
  </Step>
  <Step title="Close the loop">
    Convert stable clusters into taxonomies to enrich downstream search. Apply enrichment to write cluster_id back for filtering and faceting.
  </Step>
</Steps>

## See also

<CardGroup cols={3}>
  <Card title="Taxonomies" icon="sitemap" href="/enrichment/taxonomies">
    Promote clusters into flat or hierarchical taxonomies
  </Card>
  
  <Card title="Pipelines & Workflows" icon="diagram-project" href="/processing/pipelines">
    Combine clustering with feature extraction and enrichment
  </Card>
  
  <Card title="Webhooks" icon="webhook" href="/operations/webhooks">
    Subscribe to trigger events for monitoring and automation
  </Card>
</CardGroup>
