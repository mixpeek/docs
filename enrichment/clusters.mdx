---
title: "Clusters"
description: "Discover structure, label groups, and enrich collections with clustering"
---

<Note>
  Clustering groups similar documents to power discovery, analytics visuals, and taxonomy bootstrapping. Mixpeek runs clustering as an engine pipeline, stores artifacts (Parquet) for scale, and exposes APIs to create, execute, list, stream, and apply enrichments.
</Note>

## Overview

<CardGroup cols={2}>
  <Card title="Structure Discovery" icon="diagram-project">
    Find groups in one or more collections; optional hierarchical metadata
  </Card>
  
  <Card title="Artifacts & Scale" icon="box-archive">
    Results saved as Parquet (centroids, members) for WebGL/Arrow pipelines
  </Card>
  
  <Card title="LLM Labeling" icon="wand-magic-sparkles">
    Optionally name clusters, summaries, and keywords
  </Card>
  
  <Card title="Enrichment" icon="plus">
    Write back cluster_id membership or create derived collections
  </Card>
</CardGroup>

## How it works

<Frame>
  ```mermaid
  graph TD
    SRC[Source Collections] --> PRE[Preprocess]
    PRE --> CL[Clustering]
    CL --> POST[Postprocess]
    POST --> ART[Artifacts Parquet]
    POST --> ENR[Optional Enrichment]
  ```
</Frame>

<Steps>
  <Step title="Preprocess">
    Optional normalization and dimensionality reduction (UMAP, tSNE, etc.)
  </Step>
  <Step title="Cluster">
    Algorithms like KMeans, DBSCAN, HDBSCAN, Agglomerative, Spectral, GMM, Mean Shift, OPTICS
  </Step>
  <Step title="Postprocess">
    Centroids and stats; optional LLM labeling and hierarchical metadata
  </Step>
  <Step title="Persist">
    Parquet artifacts saved per run_id; list and stream via API
  </Step>
</Steps>

## Multimodal example

<Frame>
  ```mermaid
  graph TD
    subgraph Sources
      IMG[Images]
      VID[Videos]
      AUD[Audio]
      TXT[Text]
    end

    IMG --> FE[Feature Extraction - CLIP, Wav2Vec, e5]
    VID --> FE
    AUD --> FE
    TXT --> FE

    FE --> VEC[Vectors]
    VEC --> PRE[Preprocess - normalize and reduce]
    PRE --> CL[Cluster - KMeans or HDBSCAN]
    CL --> ART[Artifacts - Parquet]
    ART --> ENR[Enrich - cluster_id and labels]
    ENR -. optional .-> TAX[Derived Taxonomy]
  ```
</Frame>

## Create a cluster definition

- **API**: Create Cluster
- **Method**: POST
- **Path**: `/v1/clusters`
- **Reference**: [API Reference](/api-reference/clusters/create-cluster)

```bash
curl -X POST https://api.mixpeek.com/v1/clusters \
  -H "Authorization: Bearer $API_KEY" \
  -H "X-Namespace: ns_123" \
  -H "Content-Type: application/json" \
  -d '{
    "collection_ids": ["col_products_v1"],
    "cluster_name": "products_clip_hdbscan",
    "cluster_type": "vector",
    "vector_config": {
      "feature_extractor_name": "clip_vit_l_14",
      "clustering_method": "hdbscan",
      "hdbscan_parameters": {"min_cluster_size": 10, "min_samples": 5}
    },
    "llm_labeling": {"enabled": true, "model_name": "gpt-4"}
  }'
```

## Execute clustering

- **API**: Execute Clustering
- **Method**: POST
- **Path**: `/v1/clusters/execute`
- **Reference**: [API Reference](/api-reference/clusters/execute-clustering)

```bash
curl -X POST https://api.mixpeek.com/v1/clusters/execute \
  -H "Authorization: Bearer $API_KEY" \
  -H "X-Namespace: ns_123" \
  -H "Content-Type: application/json" \
  -d '{
    "collection_ids": ["col_products_v1"],
    "config": {
      "algorithm": "kmeans",
      "algorithm_params": {"n_clusters": 8, "max_iter": 300},
      "feature_vector": {"feature_address": {"extractor": "clip_vit_l_14", "version": "1.0.0"}},
      "normalize_features": true,
      "dimensionality_reduction": {"method": "umap", "n_components": 2}
    },
    "sample_size": 10000,
    "store_results": true,
    "include_members": false,
    "compute_metrics": true,
    "save_artifacts": true
  }'
```

Response includes `run_id`, metrics, and centroid summaries. Artifacts are written under a perâ€‘run S3 prefix.

## Stream artifacts for UI

- **API**: Stream Cluster Data
- **Method**: POST
- **Path**: `/v1/clusters/{cluster_id}/data`
- **Reference**: [API Reference](/api-reference/clusters/stream-cluster-data)

```bash
curl -X POST https://api.mixpeek.com/v1/clusters/CLUSTER_ID/data \
  -H "Authorization: Bearer $API_KEY" \
  -H "X-Namespace: ns_123" \
  -H "Content-Type: application/json" \
  -d '{
    "cluster_id": "cl_123",
    "include_centroids": true,
    "include_members": true,
    "limit": 1000,
    "offset": 0
  }'
```

Use this to load centroids and members for visualizations (2D/3D reductions, partitions by cluster_id).

## Apply cluster enrichment

- **API**: Apply Cluster Enrichment
- **Method**: POST
- **Path**: `/v1/clusters/enrich`
- **Reference**: [API Reference](/api-reference/clusters/apply-cluster-enrichment)

```bash
curl -X POST https://api.mixpeek.com/v1/clusters/enrich \
  -H "Authorization: Bearer $API_KEY" \
  -H "X-Namespace: ns_123" \
  -H "Content-Type: application/json" \
  -d '{
    "clustering_ids": ["cl_run_abc"],
    "source_collection_id": "col_products_v1",
    "target_collection_id": "col_products_enriched_v1",
    "batch_size": 1000,
    "parallelism": 4
  }'
```

This writes `cluster_id` membership (and optional labels) back to documents.

## Manage clusters

<CardGroup cols={2}>
  <Card title="Get Cluster" icon="magnifying-glass" href="/api-reference/clusters/get-cluster" />
  <Card title="List Clusters" icon="list" href="/api-reference/clusters/list-clusters" />
  <Card title="Delete Cluster" icon="trash" href="/api-reference/clusters/delete-cluster" />
  <Card title="Get Artifacts" icon="box-open" href="/api-reference/clusters/get-cluster-artifacts" />
  <Card title="Submit Job" icon="paper-plane" href="/api-reference/clusters/submit-clustering-job" />
</CardGroup>

## Config building blocks

<Tabs>
  <Tab title="Algorithms">
    kmeans, dbscan, hdbscan, agglomerative, spectral, gaussian_mixture, mean_shift, optics
  </Tab>
  
  <Tab title="Reduction & Normalization">
    Optional UMAP, tSNE, PCA; set <code>normalize_features</code> to true for L2/standard scaling
  </Tab>
  
  <Tab title="LLM Labeling">
    Enable labeling with provider/model and parameters to name clusters and extract keywords
  </Tab>
  
  <Tab title="Hierarchy">
    Set <code>hierarchical</code> and <code>max_hierarchy_depth</code> to compute parent_cluster_id and levels
  </Tab>
</Tabs>

## Artifacts (Parquet)

<AccordionGroup>
  <Accordion title="Centroids dataset" icon="bullseye">
    Columns include: <code>cluster_id</code>, <code>centroid_vector</code>, <code>num_members</code>, <code>variance</code>, <code>label</code>, <code>summary</code>, <code>keywords</code>, <code>feature_name</code>, <code>feature_dimensions</code>, <code>parent_cluster_id</code>, <code>hierarchy_level</code>, <code>reduction_method</code>, <code>parameters</code>, <code>algorithm</code>, <code>run_id</code>, timestamps
  </Accordion>
  <Accordion title="Members dataset" icon="users">
    Partitioned by <code>cluster_id</code>; includes <code>point_id</code>, reduced coordinates (<code>x</code>, <code>y</code>, optional <code>z</code>), and optional payload slice for filtering
  </Accordion>
</AccordionGroup>

## Best practices

<Steps>
  <Step title="Start with samples">
    Use <code>sample_size</code> for quick exploration before full runs
  </Step>
  <Step title="Label pragmatically">
    Enable LLM labeling after validating cluster separation
  </Step>
  <Step title="Persist visuals">
    Save artifacts and stream reduced coordinates for UI at scale
  </Step>
  <Step title="Close the loop">
    Convert stable clusters into taxonomies to enrich downstream search
  </Step>
</Steps>

## See also

<CardGroup cols={2}>
  <Card title="Taxonomies" icon="sitemap" href="/enrichment/taxonomies">
    Promote clusters into flat or hierarchical taxonomies
  </Card>
  
  <Card title="Pipelines & Workflows" icon="diagram-project" href="/processing/pipelines">
    Combine clustering with feature extraction and enrichment
  </Card>
</CardGroup>
