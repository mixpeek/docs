---
title: "Overview"
icon: "traffic-light"
iconType: "solid"
---

## What is Mixpeek?

[Mixpeek](https://mixpeek.com) provides vision understanding infrastructure for developers.

Using our set of APIs, you can pull out structured data out of videos and images like embeddings, objects, text, and more.

These files can come from object storage like Amazon S3 or real-time feeds like RTSP.

You can use the end-to-end indexing and retrieval APIs (Easy but Rigid) or the SDK methods directly (Flexible but Complex).

## Demo

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/wta3uQCvC28?si=nlWrkUS-ePTyYiNY"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
></iframe>

## Highlights

<CardGroup cols={2}>
  <Card title="Multimodal" icon="layer-group">
    Supports images and videos out of the box. Extract embeddings, objects,
    text, faces, and more from visual assets.
  </Card>
  <Card title="Storage Agnostic" icon="database">
    Works with various data sources including object storage e.g., AWS S3,
    real-time streams (e.g., RTSP), and databases.
  </Card>
  <Card title="Flexible" icon="puzzle-piece">
    Offers multiple methods, SDKs, and integrations for customizable workflows.
  </Card>
  <Card title="Scalable" icon="chart-line">
    Designed to handle large volumes of data and grow with your needs.
  </Card>
</CardGroup>

## Use Cases

<Note>
  Check the [use case section](/use-cases) for live demos, code, videos, and
  guides
</Note>

- [Real-Time Video Alerting](/use-cases/video-alerting): Enhance security and monitoring with intelligent video analysis.
- [Visual Discovery](/use-cases/visual-discovery): Enable users to explore and find visual content intuitively.
- [Multimodal Search](/use-cases/multimodal-search): Improve search capabilities with rich, multimodal data.
- [Content Recommendation](/use-cases/content-recommendation): Enhance recommendations using diverse data sources.
- [Media Analytics](/use-cases/media-analytics): Perform advanced analytics with structured, multimodal data.
- [Multimodal RAG](/use-cases/multimodal-rag): Power advanced AI models for more accurate and contextually relevant outputs.
- [Multimodal Generation](/use-cases/multimodal-generation): Create AI-generated content using multimodal inputs.
- [Content Organization](/use-cases/content-organization): Streamline and manage multimedia content efficiently.

### Indexing Example

Here's a sample of how you can use the indexing API:

```python
import requests
import json

url = "https://api.mixpeek.com/index/url"

payload = json.dumps({
    "url": "https://mixpeek-public-demo.s3.us-east-2.amazonaws.com/starter/jurassic_bunny.mp4",
    "collection_id": "starter",
    "metadata": {"foo": ["foo", "bar"]},
    "should_save": False,  # Instructs the service not to download, only do this if the file is immutable
    "settings": [
        {
            "split_interval": 1,  # Split intervals are how the processing is grouped (seconds)
            "read": {
                "model_id": "video-descriptor-v1"
            },
            "embed": {
                "model_id": "vuse-generic-v1"
            }
        },
        {
            "split_interval": 30,
            "transcribe": {
                "model_id": "polyglot-v1"
            }
        },
        {
            "split_interval": 120,
            "describe": {
                "model_id": "video-descriptor-v1"
            }
        }
    ]
})
headers = {
    'Authorization': 'Bearer MIXPEEK_API_KEY',
    'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)
```

This example demonstrates how to index a video file, applying various processing steps at different intervals. Let's break down the key components:

- **split_interval**: This determines how the processing is grouped in seconds. Different operations can be performed at different intervals for efficiency and granularity.

- **Methods**:
  - **read**: Extracts basic information from the video frames.
  - **embed**: Generates vector embeddings for the content.
  - **transcribe**: Converts speech to text.
  - **describe**: Generates textual descriptions of the video content.

In this example:

- Every 1 second, the video is read and embedded.
- Every 30 seconds, the audio is transcribed.
- Every 120 seconds, a description of the video content is generated.

This flexible approach allows for efficient processing tailored to your specific needs.

See the [quick start guide](/introduction/quickstart) to get started.
