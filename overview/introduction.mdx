---
title: "Introduction"
description: "The Multimodal Data Warehouse"
---

<Note>
  Welcome to the Mixpeek documentation! We're happy to empower you to build multimodal understanding applications.
</Note>

Before we dive in, let's quickly define what a **Multimodal Warehouse** is:

> Multimodal Warehouse: A specialized data platform that ingests, processes, and enables retrieval across diverse media types (text, images, videos, audio, PDFs) by extracting features and storing them in optimized collections and feature stores. It serves as the foundation for building AI-powered search and analysis applications that can work seamlessly across different content types.

Within Mixpeek, developers have access to pre-built feature extraction and retrieval stages that enables ad-hoc search and discovery across any file type.


<CardGroup cols={2}>
  <Card title="Multimodal Processing" icon="gears">
    Extract features from any file type with customizable processing pipelines
  </Card>
  
  <Card title="Intelligent Retrieval" icon="magnifying-glass">
    Search and retrieve data across modalities with advanced vector and metadata filtering
  </Card>

  <Card title="Data Enrichment" icon="layer-group">
    Apply taxonomies and clustering to organize and categorize your multimodal content
  </Card>

  <Card title="Scalable Architecture" icon="server">
    Built for performance with a flexible, component-based architecture
  </Card>
</CardGroup>


## Key Benefits

<AccordionGroup>
  <Accordion title="Unified Multimodal Platform">
    Process and analyze any content type with a single platform, eliminating the need for separate systems for different media types.

    In practice, this enables use cases that span multiple modalities like "unique faces in **videos** that mentions topics present in my **pdfs**". 
  </Accordion>

  <Accordion title="Custom Processing Pipelines">
    Define exactly how your content is processed with customizable feature extraction pipelines tailored to your specific needs.
  </Accordion>

  <Accordion title="Advanced Retrieval">
    Combine vector similarity, metadata filtering, and semantic understanding for powerful multimodal search capabilities.
  </Accordion>

  <Accordion title="Data Organization">
    Apply taxonomies and clustering to bring structure to unstructured content, making it easier to organize and discover.
  </Accordion>
</AccordionGroup>

### Always State-of-the-Art

<CardGroup cols={2}>
  <Card title="Continuous Improvement" icon="arrows-rotate">
    Mixpeek manages all feature extractors and retriever stages, continuously updating them to incorporate the latest advancements in AI and ML.
  </Card>
  
  <Card title="Tightly Coupled" icon="link">
    Our extractors and retrievers are designed as an integrated system. This tight coupling enables advanced techniques like late interaction models (e.g., <a href="https://github.com/stanford-futuredata/ColBERT">ColBERT</a>).
  </Card>
</CardGroup>

<Warning>
  Unlike traditional solutions that require rebuilding your entire index when upgrading to new models, Mixpeek performs upgrades seamlessly behind the scenes—keeping you on the cutting edge without disruption.
</Warning>

## How It Works

<Steps>
  <Step title="Upload Objects">
    Upload and store your multimodal data in buckets, organizing related files as objects
  </Step>
  <Step title="Extract Features">
    Extract features using custom pipelines with specialized extractors for different content types
  </Step>
  <Step title="Enrich Documents">
    Apply taxonomies (joins) and clustering (groups) to categorize and group related content
  </Step>
  <Step title="Retrieve & Analyze">
    Search and retrieve content using advanced multimodal search capabilities
  </Step>
</Steps>

## Getting Started

The fastest way to start using Mixpeek is to follow our [Quickstart Guide](/overview/quickstart) which will walk you through setting up your first project.

For a deeper understanding of how Mixpeek works, check out our [Core Concepts](/overview/concepts) page.

<Frame>
  ```mermaid
  graph TD
    A[Upload Content] --> B[Process & Extract Features]
    B --> C[Store in Collections]
    C --> D[Apply Taxonomies & Clustering]
    D --> E[Search & Retrieve]
    style A fill:#FC5185,stroke:#333
    style E fill:#FC5185,stroke:#333
  ```
</Frame>


## Common Use Cases

<Tabs>
  <Tab title="Multimodal Search">
    ### Cross-Modal Search Operations
    
    Organize and store content to enable efficient search across different modalities:

    **Storage Pattern**
    - Group related images, videos, and text documents in single objects
    - Store raw files alongside their extracted features
    - Maintain indexes for cross-modal querying

    **Example Object Structure**
    ```json
    {
      "product_id": "shoe_123",
      "blobs": [
        {"type": "image", "url": "front_view.jpg"},
        {"type": "image", "url": "side_view.jpg"},
        {"type": "text", "url": "description.txt"},
        {"type": "video", "url": "rotation.mp4"}
      ]
    }
    ```
  </Tab>

  <Tab title="Content Analytics">
    ### Automated Analysis Workflows
    
    Structure content to support comprehensive analytics processing:

    **Storage Pattern**
    - Raw media files paired with extracted metadata
    - Processing state tracking for each stage
    - Version control for processed outputs

    **Example Object Structure**
    ```json
    {
      "video_id": "stream_456",
      "blobs": [
        {"type": "video", "url": "source.mp4"},
        {"type": "json", "url": "scene_metadata.json"},
        {"type": "text", "url": "transcript.txt"},
        {"type": "json", "url": "analytics_results.json"}
      ]
    }
    ```
  </Tab>

  <Tab title="Training Data">
    ### ML Training Data Management
    
    Organize datasets for machine learning model training:

    **Storage Pattern**
    - Paired multimodal training examples
    - Ground truth data alongside source files
    - Version control for dataset iterations

    **Example Object Structure**
    ```json
    {
      "training_pair_id": "train_789",
      "blobs": [
        {"type": "image", "url": "input.jpg"},
        {"type": "json", "url": "labels.json"},
        {"type": "text", "url": "annotations.txt"},
        {"type": "json", "url": "metadata.json"}
      ]
    }
    ```
  </Tab>

  <Tab title="Processing Pipelines">
    ### Content Processing Workflows
    
    Manage data through multi-stage processing pipelines:

    **Storage Pattern**
    - Source files organized for parallel processing
    - Intermediate results stored with clear lineage
    - Output files linked to source data

    **Example Object Structure**
    ```json
    {
      "batch_id": "process_101",
      "blobs": [
        {"type": "video", "url": "raw_footage.mp4"},
        {"type": "json", "url": "processing_state.json"},
        {"type": "json", "url": "extracted_features.json"},
        {"type": "text", "url": "processing_logs.txt"}
      ]
    }
    ```
  </Tab>
</Tabs>


<Note>
  Each use case leverages Mixpeek's ability to process and understand relationships across different content types - from text and images to video and audio - providing a unified view of your data.
</Note>

Ready to get started? [Create your first project →](/overview/quickstart) 