---
title: "Deduplicate"
description: "Remove duplicate documents based on content similarity or field matching"
---

<Frame>
  <img src="/assets/retrievers/deduplicate.svg" alt="Deduplicate stage showing duplicate document removal" />
</Frame>

The Deduplicate stage removes duplicate or near-duplicate documents from results. It supports exact field matching, content hashing, and semantic similarity deduplication.

<Note>
  **Stage Category**: REDUCE (Aggregates/reduces document set)

  **Transformation**: N documents → M documents (where M ≤ N, duplicates removed)
</Note>

## When to Use

| Use Case | Description |
|----------|-------------|
| **Cross-source deduplication** | Same content from multiple sources |
| **Near-duplicate removal** | Slightly different versions of same doc |
| **Chunked document cleanup** | Remove overlapping chunks |
| **Result diversity** | Ensure varied search results |

## When NOT to Use

| Scenario | Recommended Alternative |
|----------|------------------------|
| Exact ID matching | Pre-filter in database |
| Large-scale dedup | Process during indexing |
| Complex similarity logic | Custom `api_call` |

## Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `method` | string | `content_hash` | Deduplication method |
| `field` | string | `null` | Field for exact/hash matching |
| `similarity_threshold` | float | `0.95` | For semantic dedup (0.0-1.0) |
| `keep` | string | `first` | Which duplicate to keep: `first`, `last`, `highest_score` |
| `content_field` | string | `content` | Field for content comparison |

## Deduplication Methods

| Method | Description | Speed | Use Case |
|--------|-------------|-------|----------|
| `exact_field` | Exact field value match | Fast | Matching IDs or hashes |
| `content_hash` | Hash-based content match | Fast | Exact content duplicates |
| `semantic` | Embedding similarity | Slow | Near-duplicates |

## Configuration Examples

<CodeGroup>
```json Content Hash Deduplication
{
  "stage_type": "reduce",
  "stage_id": "deduplicate",
  "parameters": {
    "method": "content_hash",
    "content_field": "content",
    "keep": "highest_score"
  }
}
```

```json Exact Field Match
{
  "stage_type": "reduce",
  "stage_id": "deduplicate",
  "parameters": {
    "method": "exact_field",
    "field": "metadata.source_url",
    "keep": "first"
  }
}
```

```json Semantic Deduplication
{
  "stage_type": "reduce",
  "stage_id": "deduplicate",
  "parameters": {
    "method": "semantic",
    "similarity_threshold": 0.92,
    "content_field": "content",
    "keep": "highest_score"
  }
}
```

```json Title-Based Deduplication
{
  "stage_type": "reduce",
  "stage_id": "deduplicate",
  "parameters": {
    "method": "exact_field",
    "field": "metadata.title",
    "keep": "last"
  }
}
```

```json Aggressive Near-Duplicate Removal
{
  "stage_type": "reduce",
  "stage_id": "deduplicate",
  "parameters": {
    "method": "semantic",
    "similarity_threshold": 0.85,
    "keep": "highest_score"
  }
}
```
</CodeGroup>

## Keep Strategies

| Strategy | Behavior |
|----------|----------|
| `first` | Keep first occurrence in result order |
| `last` | Keep last occurrence |
| `highest_score` | Keep document with highest relevance score |

<Tip>
  Use `highest_score` when deduplicating search results to retain the most relevant version of duplicate content.
</Tip>

## Output Schema

Documents are returned with duplicates removed:

```json
[
  {
    "document_id": "doc_123",
    "content": "Original content...",
    "score": 0.95,
    "dedup_info": {
      "is_duplicate": false,
      "cluster_size": 3
    }
  },
  {
    "document_id": "doc_789",
    "content": "Different content...",
    "score": 0.88,
    "dedup_info": {
      "is_duplicate": false,
      "cluster_size": 1
    }
  }
]
```

The `cluster_size` indicates how many duplicates were found (including the kept document).

## Performance

| Method | Latency | Memory |
|--------|---------|--------|
| `exact_field` | O(n) | Low |
| `content_hash` | O(n) | Low |
| `semantic` | O(n²) | High |

| Metric | Value |
|--------|-------|
| **exact_field/hash** | < 10ms for 100 docs |
| **semantic** | 50-200ms for 100 docs |
| **Max practical size** | 500 docs for semantic |

<Warning>
  Semantic deduplication compares all document pairs. For large result sets, use `content_hash` or limit results first.
</Warning>

## Common Pipeline Patterns

### Search + Dedup + Rerank

```json
[
  {
    "stage_type": "filter",
    "stage_id": "semantic_search",
    "parameters": {
      "query": "{{INPUT.query}}",
      "vector_index": "text_extractor_v1_embedding",
      "top_k": 100
    }
  },
  {
    "stage_type": "reduce",
    "stage_id": "deduplicate",
    "parameters": {
      "method": "content_hash",
      "keep": "highest_score"
    }
  },
  {
    "stage_type": "sort",
    "stage_id": "rerank",
    "parameters": {
      "model": "bge-reranker-v2-m3",
      "top_n": 10
    }
  }
]
```

### Multi-Source Search with Dedup

```json
[
  {
    "stage_type": "filter",
    "stage_id": "hybrid_search",
    "parameters": {
      "query": "{{INPUT.query}}",
      "vector_index": "text_extractor_v1_embedding",
      "top_k": 50
    }
  },
  {
    "stage_type": "apply",
    "stage_id": "external_web_search",
    "parameters": {
      "query": "{{INPUT.query}}",
      "num_results": 20
    }
  },
  {
    "stage_type": "reduce",
    "stage_id": "deduplicate",
    "parameters": {
      "method": "semantic",
      "similarity_threshold": 0.90,
      "keep": "highest_score"
    }
  }
]
```

### Chunk-Level Deduplication

```json
[
  {
    "stage_type": "filter",
    "stage_id": "semantic_search",
    "parameters": {
      "query": "{{INPUT.query}}",
      "vector_index": "text_extractor_v1_embedding",
      "top_k": 100
    }
  },
  {
    "stage_type": "reduce",
    "stage_id": "deduplicate",
    "parameters": {
      "method": "exact_field",
      "field": "metadata.parent_document_id",
      "keep": "highest_score"
    }
  },
  {
    "stage_type": "reduce",
    "stage_id": "limit",
    "parameters": {
      "limit": 10
    }
  }
]
```

## How Each Method Works

### exact_field

Groups documents by exact field value match:

```
doc1.metadata.url = "https://example.com/page1"
doc2.metadata.url = "https://example.com/page1"  <- duplicate
doc3.metadata.url = "https://example.com/page2"
```

### content_hash

Computes hash of content field:

```
hash(doc1.content) = "abc123"
hash(doc2.content) = "abc123"  <- duplicate (same hash)
hash(doc3.content) = "def456"
```

### semantic

Computes embedding similarity between all pairs:

```
similarity(doc1, doc2) = 0.97  <- duplicates (> 0.95 threshold)
similarity(doc1, doc3) = 0.42  <- not duplicates
similarity(doc2, doc3) = 0.45  <- not duplicates
```

## Error Handling

| Error | Behavior |
|-------|----------|
| Missing field | Document treated as unique |
| Empty content | Hash comparison skipped |
| Embedding failure | Falls back to content_hash |

## Related

- [Limit](/retrieval/stages/limit) - Simple result count control
- [Rerank](/retrieval/stages/rerank) - Re-score after dedup
- [Semantic Search](/retrieval/stages/semantic-search) - Initial retrieval
