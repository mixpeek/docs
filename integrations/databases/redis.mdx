---
title: "Redis"
icon: "database"
iconType: "solid"
---

## Video

We'll be using `vuse-generic-v1` to build a collection of 1 second interval video chunks into a 768 dimension embedding collection.

<Note>
  You'll need to have Redis installed with the RediSearch module. We'll create
  an index called `video_idx` for the video embeddings.
</Note>

### Ingest

```python
from mixpeek import Mixpeek
from redis import Redis
from redis.commands.search.field import VectorField
from redis.commands.search.indexDefinition import IndexDefinition, IndexType

# Initialize the Mixpeek client with your API key
mixpeek = Mixpeek("YOUR_API_KEY")

# Initialize Redis client
redis_client = Redis(host='localhost', port=6379, db=0)

# Create index for video embeddings
video_schema = (
    VectorField("embedding", "HNSW", {"TYPE": "FLOAT32", "DIM": 768, "DISTANCE_METRIC": "COSINE"})
)
redis_client.ft("video_idx").create_index(
    fields = [video_schema],
    definition = IndexDefinition(prefix=["video:"], index_type=IndexType.HASH)
)

# Process video chunks
processed_chunks = mixpeek.tools.video.process(
    video_source="https://mixpeek-public-demo.s3.us-east-2.amazonaws.com/redis/Jurassic+Park+(2).mp4",
    chunk_interval=1, # 1 second intervals
    resolution=[720, 1280]
)

for i, chunk in enumerate(processed_chunks):
    print(f"Processing video chunk: {i}")

    # embed each chunk
    embed_response = mixpeek.embed.video(
        model_id="vuse-generic-v1",
        input=chunk['base64_chunk'],
        input_type="base64"
    )

    # Add to Redis
    redis_client.hset(f"video:{i}", mapping={
        "embedding": embed_response['embedding'],
        "start_time": chunk["start_time"],
        "end_time": chunk["end_time"]
    })
```

### Text Query

```python
query = "two boys inside a car"

embed_response = mixpeek.embed.video(
    model_id="vuse-generic-v1",
    input=query,
    input_type="text"
)

query_vector = embed_response['embedding']
results = redis_client.ft("video_idx").search(
    f"*=>[KNN 10 @embedding $query_vector AS score]",
    query_params = {"query_vector": query_vector}
)

for result in results.docs:
    print(result)
```

### Video Query

```python
# we'll use a cartoon version of jurassic park
file_url = "https://mixpeek-public-demo.s3.us-east-2.amazonaws.com/redis/rabbit-jurassic.mp4"

embed_response = mixpeek.embed.video(
    model_id="vuse-generic-v1",
    input=file_url,
    input_type="url"
)

query_vector = embed_response['embedding']
results = redis_client.ft("video_idx").search(
    f"*=>[KNN 10 @embedding $query_vector AS score]",
    query_params = {"query_vector": query_vector}
)

for result in results.docs:
    print(result)
```

## Image

We'll be using `openai-clip-vit-base-patch32` to build a collection of image embeddings with 512 dimensions.

<Note>
  You'll need to have Redis installed with the RediSearch module. We'll create
  an index called `image_idx` for the image embeddings.
</Note>

### Ingest

```python
from mixpeek import Mixpeek
from redis import Redis
from redis.commands.search.field import VectorField
from redis.commands.search.indexDefinition import IndexDefinition, IndexType

# Initialize the Mixpeek client with your API key
mixpeek = Mixpeek("YOUR_API_KEY")

# Initialize Redis client
redis_client = Redis(host='localhost', port=6379, db=0)

# Create index for image embeddings
image_schema = (
    VectorField("embedding", "HNSW", {"TYPE": "FLOAT32", "DIM": 512, "DISTANCE_METRIC": "COSINE"})
)
redis_client.ft("image_idx").create_index(
    fields = [image_schema],
    definition = IndexDefinition(prefix=["image:"], index_type=IndexType.HASH)
)

# List of image URLs to process
image_urls = [
    "https://mixpeek-public-demo.s3.us-east-2.amazonaws.com/redis/image1.jpg",
    "https://mixpeek-public-demo.s3.us-east-2.amazonaws.com/redis/image2.jpg",
    "https://mixpeek-public-demo.s3.us-east-2.amazonaws.com/redis/image3.jpg",
    "https://mixpeek-public-demo.s3.us-east-2.amazonaws.com/redis/image4.jpg"
]

for i, url in enumerate(image_urls):
    print(f"Processing image: {url}")

    # Embed each image
    embed_response = mixpeek.embed.image(
        model_id="openai-clip-vit-base-patch32",
        input=url,
        input_type="url"
    )

    # Add to Redis
    redis_client.hset(f"image:{i}", mapping={
        "embedding": embed_response['embedding'],
        "image_url": url
    })
```

### Text Query

```python
query = "a cat sitting on a windowsill"

embed_response = mixpeek.embed.image(
    model_id="openai-clip-vit-base-patch32",
    input=query,
    input_type="text"
)

query_vector = embed_response['embedding']
results = redis_client.ft("image_idx").search(
    f"*=>[KNN 5 @embedding $query_vector AS score]",
    query_params = {"query_vector": query_vector}
)

for result in results.docs:
    print(result)
```

### Image Query

```python
# Use an image from the same bucket as a query
query_image = "https://mixpeek-public-demo.s3.us-east-2.amazonaws.com/redis/query_image.jpg"

embed_response = mixpeek.embed.image(
    model_id="openai-clip-vit-base-patch32",
    input=query_image,
    input_type="url"
)

query_vector = embed_response['embedding']
results = redis_client.ft("image_idx").search(
    f"*=>[KNN 5 @embedding $query_vector AS score]",
    query_params = {"query_vector": query_vector}
)

for result in results.docs:
    print(result)
```
