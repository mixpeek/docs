---
title: "Product Updates"
description: "New updates and improvements"
mode: "wide"
---

<Update label="1-1-2025">

### Feature: Averaged Multi-Input Embeddings

Added support for averaging multiple embeddings when the same embedding model is used across different inputs. This allows for creating more representative embeddings by combining multiple perspectives or descriptions of the same content.

#### Examples

```python
# Before: Later inputs would overwrite earlier ones
settings = [
    EmbeddingRequest(
        type="text",
        value="a dog running",
        embedding_model="clip"
    ),
    EmbeddingRequest(
        type="text",
        value="canine in motion",
        embedding_model="clip"
    )
]
# Result:
# {
#     "vectors": {
#         "clip": [0.2, 0.3, 0.5]  # Only keeps last embedding
#     },
#     "original_values": {
#         "clip": "canine in motion"  # Only keeps last value
#     }
# }

# After: Embeddings are averaged and all inputs are preserved
settings = [
    EmbeddingRequest(
        type="text",
        value="a dog running",
        embedding_model="clip"
    ),
    EmbeddingRequest(
        type="text",
        value="canine in motion",
        embedding_model="clip"
    )
]
# Result:
# {
#     "vectors": {
#         "clip": [0.25, 0.35, 0.45]  # Average of both embeddings
#     },
#     "original_values": {
#         "clip": [
#             {
#                 "type": "text",
#                 "value": "a dog running",
#                 "embedding_model": "clip"
#             },
#             {
#                 "type": "text",
#                 "value": "canine in motion",
#                 "embedding_model": "clip"
#             }
#         ]
#     }
# }
```

### Use Cases
- Combine multiple text descriptions to create a more comprehensive embedding
- Mix image and text embeddings in the same vector space
- Preserve all original inputs for reference and debugging

### Technical Details
- Embeddings from the same model are averaged element-wise
- All original `EmbeddingRequest` objects are stored in `original_values`
- Works with both text and image inputs
- Compatible with all supported embedding providers (Vertex, SageMaker)

</Update>

<Update label="12-29-2024">
### Enhancement: Read-Only API Keys

You can now create API keys with granular permissions: read-only, write-only, or read-write access. This enables building secure client-side applications by restricting access to sensitive operations.

#### Permission Levels
- **Read-only**: Allows access to GET endpoints only (e.g., `/api/v1/search`, `/api/v1/items/{id}`)
- **Write-only**: Enables POST, PUT, DELETE operations (e.g., `/api/v1/items`, `/api/v1/batch`)
- **Read-write**: Full access to all endpoints

For a complete list of endpoints and their required permissions, see our [API Reference](/api-reference).

#### How to Configure
1. Navigate to Settings > API Keys in the studio portal
2. Create a new key or edit an existing one
3. Select the desired permission level
4. Save changes
</Update>


<Update label="12-28-2024" description="0.15.2">
  ### Breaking Change: vector_index renamed to embedding_models

  We've renamed the `vector_index` configuration parameter to `embedding_models` to better reflect its purpose and align with industry terminology. This change affects all API endpoints and configuration files that previously used `vector_index`.

  ### What You Need To Do
  * Update all references to `vector_index` in your configuration files to use `embedding_models`
  * Review and update any API calls that included the `vector_index` parameter
  * Check your integration tests and documentation for any mentions of the old parameter name

  For a complete list of supported embedding models and migration guidance, please refer to our [available models documentation](/overview/models/available-models).
</Update>

<Update label="12-12-2024" description="0.15.0">
  ## Taxonomies and Clustering

  Introducing powerful taxonomies to organize and classify your content using multimodal AI. Create hierarchical categories that understand both text and images, enabling intelligent content organization at scale.

  ### Key Features
  * **Flexible Hierarchy** - Build nested categories with independent node matching and confidence scoring
  * **Multimodal Support** - Define taxonomy nodes using text, images, or both
  * **Test-Driven Development** - Validate taxonomies on sample data before production deployment
  * **Automated Classification** - Automatically categorize new content during ingestion
  * **Manual Controls** - Override or supplement AI classifications with manual assignments

  Check out our [taxonomy documentation](/ingestion/entities/taxonomies) to get started organizing your content library.
</Update>